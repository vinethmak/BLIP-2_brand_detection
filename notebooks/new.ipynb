{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "60b1a2fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Generated Caption: louis vu mono cherry mini\n",
      " Candidate Words: ['mono', 'louis', 'mini', 'cherry']\n",
      "\n",
      "ðŸ·ï¸ Predicted Brand: cherry\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from PIL import Image\n",
    "import clip\n",
    "from transformers import BlipProcessor, BlipForConditionalGeneration\n",
    "\n",
    "# -----------------------------\n",
    "# 1. Load models\n",
    "# -----------------------------\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# Load BLIP for image captioning\n",
    "blip_processor = BlipProcessor.from_pretrained(\"Salesforce/blip-image-captioning-base\")\n",
    "blip_model = BlipForConditionalGeneration.from_pretrained(\"Salesforce/blip-image-captioning-base\").to(device)\n",
    "\n",
    "# Load CLIP\n",
    "clip_model, clip_preprocess = clip.load(\"ViT-B/32\", device=device)\n",
    "\n",
    "# -----------------------------\n",
    "# 2. Load and process the image\n",
    "# -----------------------------\n",
    "image_path = \"download (1).jpg\" \n",
    "raw_image = Image.open(image_path).convert('RGB')\n",
    "\n",
    "# BLIP captioning\n",
    "blip_inputs = blip_processor(raw_image, return_tensors=\"pt\").to(device)\n",
    "caption_ids = blip_model.generate(**blip_inputs)\n",
    "caption = blip_processor.decode(caption_ids[0], skip_special_tokens=True)\n",
    "\n",
    "print(\" Generated Caption:\", caption)\n",
    "\n",
    "# -----------------------------\n",
    "# 3. Extract candidate keywords\n",
    "# -----------------------------\n",
    "# Simple method: use unique words (you can replace with spaCy or LLM)\n",
    "stopwords = set([\"a\", \"an\", \"the\", \"with\", \"and\", \"on\", \"of\", \"in\", \"this\", \"that\", \"it\", \"is\", \"to\"])\n",
    "words = list(set(caption.lower().split()))\n",
    "candidates = [word.strip('.,') for word in words if word not in stopwords and len(word) > 2]\n",
    "\n",
    "if not candidates:\n",
    "    raise ValueError(\" No brand candidates found. Try a clearer image or use spaCy/NER for better results.\")\n",
    "\n",
    "print(\" Candidate Words:\", candidates)\n",
    "\n",
    "# -----------------------------\n",
    "# 4. Use CLIP to find best-matching word\n",
    "# -----------------------------\n",
    "# Create prompts\n",
    "prompts = [f\"a product from {word}\" for word in candidates]\n",
    "text_tokens = clip.tokenize(prompts).to(device)\n",
    "image_input = clip_preprocess(raw_image).unsqueeze(0).to(device)\n",
    "\n",
    "# Encode with CLIP\n",
    "with torch.no_grad():\n",
    "    image_features = clip_model.encode_image(image_input)\n",
    "    text_features = clip_model.encode_text(text_tokens)\n",
    "    image_features /= image_features.norm(dim=-1, keepdim=True)\n",
    "    text_features /= text_features.norm(dim=-1, keepdim=True)\n",
    "\n",
    "    similarity = (image_features @ text_features.T).squeeze(0)\n",
    "    best_idx = similarity.argmax().item()\n",
    "\n",
    "# -----------------------------\n",
    "# 5. Output result\n",
    "# -----------------------------\n",
    "print(f\"\\nðŸ·ï¸ Predicted Brand: {candidates[best_idx]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "340f2307",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“ Generated Caption: puer puer puer puer puer puer puer puer puer puer\n",
      "ðŸ” Candidate Keywords: ['puer']\n",
      "\n",
      "ðŸ·ï¸ Predicted Brand or Keyword: **puer**\n",
      "ðŸ’¡ CLIP Prompt Used: 'a product from puer'\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from PIL import Image\n",
    "import clip\n",
    "from transformers import BlipProcessor, BlipForConditionalGeneration\n",
    "\n",
    "# -----------------------------\n",
    "# 1. Load models\n",
    "# -----------------------------\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# Load BLIP for image captioning\n",
    "blip_processor = BlipProcessor.from_pretrained(\"Salesforce/blip-image-captioning-base\")\n",
    "blip_model = BlipForConditionalGeneration.from_pretrained(\"Salesforce/blip-image-captioning-base\").to(device)\n",
    "\n",
    "# Load CLIP for similarity checking\n",
    "clip_model, clip_preprocess = clip.load(\"ViT-B/32\", device=device)\n",
    "\n",
    "# -----------------------------\n",
    "# 2. Load and process the image\n",
    "# -----------------------------\n",
    "image_path = \"download4.jpg\"  \n",
    "raw_image = Image.open(image_path).convert('RGB')\n",
    "\n",
    "# Generate caption using BLIP\n",
    "blip_inputs = blip_processor(raw_image, return_tensors=\"pt\").to(device)\n",
    "caption_ids = blip_model.generate(**blip_inputs)\n",
    "caption = blip_processor.decode(caption_ids[0], skip_special_tokens=True)\n",
    "\n",
    "print(\"ðŸ“ Generated Caption:\", caption)\n",
    "\n",
    "# -----------------------------\n",
    "# 3. Extract candidate words\n",
    "# -----------------------------\n",
    "stopwords = set([\n",
    "    \"a\", \"an\", \"the\", \"with\", \"and\", \"on\", \"of\", \"in\", \"this\", \"that\", \"it\", \"is\", \"to\", \"for\", \"from\", \"by\"\n",
    "])\n",
    "words = list(set(caption.lower().split()))\n",
    "candidates = [word.strip('.,') for word in words if word not in stopwords and len(word) > 2]\n",
    "\n",
    "if not candidates:\n",
    "    raise ValueError(\"No brand candidates found. Try a clearer image or improve captioning.\")\n",
    "\n",
    "print(\"ðŸ” Candidate Keywords:\", candidates)\n",
    "\n",
    "# -----------------------------\n",
    "# 4. Use CLIP to score candidates\n",
    "# -----------------------------\n",
    "# Build prompts for CLIP\n",
    "prompts = [f\"a product from {word}\" for word in candidates]\n",
    "text_tokens = clip.tokenize(prompts).to(device)\n",
    "image_input = clip_preprocess(raw_image).unsqueeze(0).to(device)\n",
    "\n",
    "# Encode image and text using CLIP\n",
    "with torch.no_grad():\n",
    "    image_features = clip_model.encode_image(image_input)\n",
    "    text_features = clip_model.encode_text(text_tokens)\n",
    "\n",
    "    # Normalize features\n",
    "    image_features /= image_features.norm(dim=-1, keepdim=True)\n",
    "    text_features /= text_features.norm(dim=-1, keepdim=True)\n",
    "\n",
    "    # Compute cosine similarity\n",
    "    similarity = (image_features @ text_features.T).squeeze(0)\n",
    "    best_idx = similarity.argmax().item()\n",
    "\n",
    "# -----------------------------\n",
    "# 5. Output best match\n",
    "# -----------------------------\n",
    "print(f\"\\nðŸ·ï¸ Predicted Brand or Keyword: **{candidates[best_idx]}**\")\n",
    "print(f\"ðŸ’¡ CLIP Prompt Used: '{prompts[best_idx]}'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "992a0131",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated Caption: nike ky ky ky ky ky ky ky ky ky ky ky ky ky ky ky ky ky ky ky\n",
      "No named entities found. Falling back to keywords.\n",
      "Candidate Brand Entities: ['ky', 'nike']\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from PIL import Image\n",
    "import clip\n",
    "import spacy\n",
    "from transformers import BlipProcessor, BlipForConditionalGeneration\n",
    "\n",
    "# -----------------------------\n",
    "# 1. Load models\n",
    "# -----------------------------\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# Load BLIP model\n",
    "blip_processor = BlipProcessor.from_pretrained(\"Salesforce/blip-image-captioning-base\")\n",
    "blip_model = BlipForConditionalGeneration.from_pretrained(\"Salesforce/blip-image-captioning-base\").to(device)\n",
    "\n",
    "# Load CLIP model\n",
    "clip_model, clip_preprocess = clip.load(\"ViT-B/32\", device=device)\n",
    "\n",
    "\n",
    "# Load spaCy NER model\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# -----------------------------\n",
    "# 2. Load and caption image\n",
    "# -----------------------------\n",
    "image_path = \"download5.jpg\"  \n",
    "raw_image = Image.open(image_path).convert('RGB')\n",
    "\n",
    "# Generate caption with BLIP\n",
    "inputs = blip_processor(raw_image, return_tensors=\"pt\").to(device)\n",
    "caption_ids = blip_model.generate(**inputs)\n",
    "caption = blip_processor.decode(caption_ids[0], skip_special_tokens=True)\n",
    "\n",
    "print(\"Generated Caption:\", caption)\n",
    "\n",
    "# -----------------------------\n",
    "# 3. Extract named entities using spaCy\n",
    "# -----------------------------\n",
    "doc = nlp(caption)\n",
    "entities = [ent.text for ent in doc.ents if ent.label_ in [\"ORG\", \"PRODUCT\", \"PERSON\", \"GPE\"]]\n",
    "\n",
    "if not entities:\n",
    "    print(\"No named entities found. Falling back to keywords.\")\n",
    "    entities = list(set(caption.lower().split()))  # fallback to generic words\n",
    "\n",
    "print(\"Candidate Brand Entities:\", entities)\n",
    "\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# # 4. Rerank using CLIP\n",
    "# # -----------------------------\n",
    "# prompts = [f\"a product from {ent}\" for ent in entities]\n",
    "# text_tokens = clip.tokenize(prompts).to(device)\n",
    "# image_input = clip_preprocess(raw_image).unsqueeze(0).to(device)\n",
    "\n",
    "# # CLIP similarity\n",
    "# with torch.no_grad():\n",
    "#     image_features = clip_model.encode_image(image_input)\n",
    "#     text_features = clip_model.encode_text(text_tokens)\n",
    "#     image_features /= image_features.norm(dim=-1, keepdim=True)\n",
    "#     text_features /= text_features.norm(dim=-1, keepdim=True)\n",
    "\n",
    "#     similarity = (image_features @ text_features.T).squeeze(0)\n",
    "#     best_idx = similarity.argmax().item()\n",
    "\n",
    "# -----------------------------\n",
    "# 5. Output\n",
    "# -----------------------------\n",
    "# print(f\" Predicted Brand (NER + CLIP): {entities[best_idx]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c3a11a34",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nike\n"
     ]
    }
   ],
   "source": [
    "print(entities[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "cba46b03",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated Caption: nike zoom zoom low gs\n",
      "No named entities found. Falling back to keywords.\n",
      "Candidate Brand Entities: ['nike', 'zoom', 'gs', 'low']\n",
      "nike\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from PIL import Image\n",
    "import spacy\n",
    "from transformers import BlipProcessor, BlipForConditionalGeneration\n",
    "\n",
    "# -----------------------------\n",
    "# 1. Load models\n",
    "# -----------------------------\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# Load BLIP model\n",
    "blip_processor = BlipProcessor.from_pretrained(\"Salesforce/blip-image-captioning-base\")\n",
    "blip_model = BlipForConditionalGeneration.from_pretrained(\"Salesforce/blip-image-captioning-base\").to(device)\n",
    "\n",
    "# Load spaCy NER model\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# -----------------------------\n",
    "# 2. Load and caption image\n",
    "# -----------------------------\n",
    "image_path = \"images.jpg\"  \n",
    "raw_image = Image.open(image_path).convert('RGB')\n",
    "\n",
    "# Generate caption with BLIP\n",
    "inputs = blip_processor(raw_image, return_tensors=\"pt\").to(device)\n",
    "caption_ids = blip_model.generate(**inputs)\n",
    "caption = blip_processor.decode(caption_ids[0], skip_special_tokens=True)\n",
    "\n",
    "print(\"Generated Caption:\", caption)\n",
    "\n",
    "# -----------------------------\n",
    "# 3. Extract named entities using spaCy\n",
    "# -----------------------------\n",
    "doc = nlp(caption)\n",
    "entities = [ent.text for ent in doc.ents if ent.label_ in [\"ORG\", \"PRODUCT\", \"PERSON\", \"GPE\"]]\n",
    "\n",
    "if not entities:\n",
    "    print(\"No named entities found. Falling back to keywords.\")\n",
    "    entities = list(set(caption.lower().split()))  # fallback to generic words\n",
    "\n",
    "print(\"Candidate Brand Entities:\", entities)\n",
    "\n",
    "print(entities[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "efd4d9c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated Caption: nike zoom zoom low gs\n",
      "No named entities found. Falling back to keyword-based heuristic.\n",
      "Candidate Brand Entities: ['zoom']\n",
      "Predicted Brand Name: zoom\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from PIL import Image\n",
    "import spacy\n",
    "from transformers import BlipProcessor, BlipForConditionalGeneration\n",
    "\n",
    "# -----------------------------\n",
    "# 1. Load models\n",
    "# -----------------------------\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# Load BLIP model\n",
    "blip_processor = BlipProcessor.from_pretrained(\"Salesforce/blip-image-captioning-base\")\n",
    "blip_model = BlipForConditionalGeneration.from_pretrained(\"Salesforce/blip-image-captioning-base\").to(device)\n",
    "\n",
    "# Load spaCy NER model\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# -----------------------------\n",
    "# 2. Load and caption image\n",
    "# -----------------------------\n",
    "image_path = \"images.jpg\"  \n",
    "raw_image = Image.open(image_path).convert('RGB')\n",
    "\n",
    "# Generate caption with BLIP\n",
    "inputs = blip_processor(raw_image, return_tensors=\"pt\").to(device)\n",
    "caption_ids = blip_model.generate(**inputs)\n",
    "caption = blip_processor.decode(caption_ids[0], skip_special_tokens=True).strip()\n",
    "\n",
    "print(\"Generated Caption:\", caption)\n",
    "\n",
    "# Add punctuation to help spaCy NER (optional but helpful)\n",
    "if not caption.endswith('.'):\n",
    "    caption += '.'\n",
    "\n",
    "# -----------------------------\n",
    "# 3. Extract named entities using spaCy\n",
    "# -----------------------------\n",
    "doc = nlp(caption)\n",
    "entities = [ent.text for ent in doc.ents if ent.label_ in [\"ORG\", \"PRODUCT\", \"PERSON\", \"GPE\"]]\n",
    "\n",
    "# -----------------------------\n",
    "# 4. Fallback if no entities found\n",
    "# -----------------------------\n",
    "if not entities:\n",
    "    print(\"No named entities found. Falling back to keyword-based heuristic.\")\n",
    "\n",
    "    # Tokenize caption\n",
    "    tokens = caption.lower().replace('.', '').split()\n",
    "\n",
    "    # Optional: remove common meaningless words\n",
    "    ignored_words = {'the', 'a', 'an', 'is', 'low', 'gs', 'shoe', 'shoes', 'color', 'white', 'black'}\n",
    "    filtered_tokens = [t for t in tokens if t not in ignored_words and len(t) > 1]\n",
    "\n",
    "    # Frequency-based fallback\n",
    "    if filtered_tokens:\n",
    "        token_freq = {t: filtered_tokens.count(t) for t in set(filtered_tokens)}\n",
    "        sorted_tokens = sorted(token_freq.items(), key=lambda x: (-x[1], caption.lower().find(x[0])))\n",
    "        fallback_brand = sorted_tokens[0][0]\n",
    "        entities = [fallback_brand]\n",
    "    else:\n",
    "        print(\"No valid tokens found for fallback.\")\n",
    "        entities = []\n",
    "\n",
    "# -----------------------------\n",
    "# 5. Print results\n",
    "# -----------------------------\n",
    "if entities:\n",
    "    print(\"Candidate Brand Entities:\", entities)\n",
    "    print(\"Predicted Brand Name:\", entities[0])\n",
    "else:\n",
    "    print(\"Brand name could not be identified.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "33cc9b39",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated Caption: louis vu mono cherry mini\n",
      "Cleaned Tokens: ['louis', 'vu', 'mono', 'cherry', 'mini']\n",
      "NER Entities: ['louis vu mono']\n",
      "Candidate Brand Entities: ['mono', 'louis', 'vu', 'louis vu mono', 'mini', 'cherry']\n",
      "Predicted Brand (top candidate): mono\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from PIL import Image\n",
    "import spacy\n",
    "from transformers import BlipProcessor, BlipForConditionalGeneration\n",
    "import re\n",
    "from spacy.lang.en.stop_words import STOP_WORDS\n",
    "\n",
    "# Optional: Add known brand names for matching (can be extended)\n",
    "known_brands = [\n",
    "    \"nike\", \"adidas\", \"puma\", \"reebok\", \"new balance\", \"asics\", \"under armour\",\n",
    "    \"vans\", \"converse\", \"fila\", \"skechers\", \"jordans\", \"balenciaga\", \"gucci\"\n",
    "]\n",
    "\n",
    "# -----------------------------\n",
    "# 1. Load models\n",
    "# -----------------------------\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# Load BLIP model\n",
    "blip_processor = BlipProcessor.from_pretrained(\"Salesforce/blip-image-captioning-base\")\n",
    "blip_model = BlipForConditionalGeneration.from_pretrained(\"Salesforce/blip-image-captioning-base\").to(device)\n",
    "\n",
    "# Load spaCy NER\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# -----------------------------\n",
    "# 2. Load and caption image\n",
    "# -----------------------------\n",
    "image_path = \"download (1).jpg\"\n",
    "raw_image = Image.open(image_path).convert(\"RGB\")\n",
    "\n",
    "# Generate caption\n",
    "inputs = blip_processor(raw_image, return_tensors=\"pt\").to(device)\n",
    "caption_ids = blip_model.generate(**inputs)\n",
    "caption = blip_processor.decode(caption_ids[0], skip_special_tokens=True)\n",
    "\n",
    "print(\"Generated Caption:\", caption)\n",
    "\n",
    "# -----------------------------\n",
    "# 3. Preprocess caption text\n",
    "# -----------------------------\n",
    "def clean_text(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)  # remove punctuation\n",
    "    return text\n",
    "\n",
    "clean_caption = clean_text(caption)\n",
    "caption_tokens = [word for word in clean_caption.split() if word not in STOP_WORDS]\n",
    "\n",
    "# -----------------------------\n",
    "# 4. Named Entity Recognition\n",
    "# -----------------------------\n",
    "doc = nlp(caption)\n",
    "entities = [ent.text.lower() for ent in doc.ents if ent.label_ in [\"ORG\", \"PRODUCT\", \"GPE\", \"PERSON\"]]\n",
    "\n",
    "# Combine NER and cleaned tokens\n",
    "candidate_entities = list(set(entities + caption_tokens))\n",
    "\n",
    "# -----------------------------\n",
    "# 5. Match against known brand list\n",
    "# -----------------------------\n",
    "matched_brands = [word for word in candidate_entities if word in known_brands]\n",
    "\n",
    "# -----------------------------\n",
    "# 6. Output result\n",
    "# -----------------------------\n",
    "print(\"Cleaned Tokens:\", caption_tokens)\n",
    "print(\"NER Entities:\", entities)\n",
    "print(\"Candidate Brand Entities:\", candidate_entities)\n",
    "\n",
    "if matched_brands:\n",
    "    print(f\"Predicted Brand (matched): {matched_brands[0]}\")\n",
    "elif candidate_entities:\n",
    "    print(f\"Predicted Brand (top candidate): {candidate_entities[0]}\")\n",
    "else:\n",
    "    print(\"Predicted Brand: Unknown\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "c21552d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated Caption: puer puer puer puer puer puer puer puer puer puer\n",
      "NER Entities: ['puer puer', 'puer puer', 'puer puer', 'puer puer', 'puer puer']\n",
      "Caption Tokens: ['puer', 'puer', 'puer', 'puer', 'puer', 'puer', 'puer', 'puer', 'puer', 'puer']\n",
      "Candidate Brand Entities: ['puer puer', 'puer']\n",
      "Predicted Brand (NER): puer puer\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from PIL import Image\n",
    "import spacy\n",
    "from transformers import BlipProcessor, BlipForConditionalGeneration\n",
    "import re\n",
    "from spacy.lang.en.stop_words import STOP_WORDS\n",
    "\n",
    "# -----------------------------\n",
    "# 1. Load models\n",
    "# -----------------------------\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# Load BLIP model for image captioning\n",
    "blip_processor = BlipProcessor.from_pretrained(\"Salesforce/blip-image-captioning-base\")\n",
    "blip_model = BlipForConditionalGeneration.from_pretrained(\"Salesforce/blip-image-captioning-base\").to(device)\n",
    "\n",
    "# Load spaCy NER model\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# -----------------------------\n",
    "# 2. Load and caption image\n",
    "# -----------------------------\n",
    "image_path = \"download4.jpg\"\n",
    "raw_image = Image.open(image_path).convert(\"RGB\")\n",
    "\n",
    "inputs = blip_processor(raw_image, return_tensors=\"pt\").to(device)\n",
    "caption_ids = blip_model.generate(**inputs)\n",
    "caption = blip_processor.decode(caption_ids[0], skip_special_tokens=True)\n",
    "\n",
    "print(\"Generated Caption:\", caption)\n",
    "\n",
    "# -----------------------------\n",
    "# 3. Preprocess caption\n",
    "# -----------------------------\n",
    "def clean_text(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)  # remove punctuation\n",
    "    return text\n",
    "\n",
    "clean_caption = clean_text(caption)\n",
    "tokens = [word for word in clean_caption.split() if word not in STOP_WORDS]\n",
    "\n",
    "# -----------------------------\n",
    "# 4. Named Entity Recognition\n",
    "# -----------------------------\n",
    "doc = nlp(caption)\n",
    "ner_entities = [ent.text.lower() for ent in doc.ents if ent.label_ in [\"ORG\", \"PRODUCT\", \"GPE\", \"PERSON\"]]\n",
    "\n",
    "# -----------------------------\n",
    "# 5. Merge + Filter Duplicates\n",
    "# -----------------------------\n",
    "candidates = list(dict.fromkeys(ner_entities + tokens))  # preserves order, removes duplicates\n",
    "\n",
    "# -----------------------------\n",
    "# 6. Select Best Candidate\n",
    "# -----------------------------\n",
    "if ner_entities:\n",
    "    predicted_brand = ner_entities[0]  # prioritize named entity\n",
    "    source = \"NER\"\n",
    "elif candidates:\n",
    "    predicted_brand = candidates[0]  # fallback to clean keyword\n",
    "    source = \"Heuristic fallback\"\n",
    "else:\n",
    "    predicted_brand = \"Unknown\"\n",
    "    source = \"None\"\n",
    "\n",
    "# -----------------------------\n",
    "# 7. Output\n",
    "# -----------------------------\n",
    "print(\"NER Entities:\", ner_entities)\n",
    "print(\"Caption Tokens:\", tokens)\n",
    "print(\"Candidate Brand Entities:\", candidates)\n",
    "print(f\"Predicted Brand ({source}): {predicted_brand}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5488c944",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from PIL import Image\n",
    "import spacy\n",
    "import re\n",
    "from transformers import Blip2Processor, Blip2ForConditionalGeneration\n",
    "from spacy.lang.en.stop_words import STOP_WORDS\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "26f70b46",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4ab95f8823e94563bc6be13bfe347838",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# -----------------------------\n",
    "# 1. Load Models\n",
    "# -----------------------------\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# Load BLIP-2 (OPT version - not instruction-tuned)\n",
    "processor = Blip2Processor.from_pretrained(\"Salesforce/blip2-opt-2.7b\")\n",
    "model = Blip2ForConditionalGeneration.from_pretrained(\"Salesforce/blip2-opt-2.7b\").to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ab119873",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated Caption: the women's reebok crossfit running shoe is black and pink\n",
      "\n",
      "NER Entities: []\n",
      "Tokens: ['womens', 'reebok', 'crossfit', 'running', 'shoe', 'black', 'pink']\n",
      "Candidates: ['womens', 'reebok', 'crossfit', 'running', 'shoe', 'black', 'pink']\n",
      "Predicted Brand (Heuristic): womens\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Load spaCy English model for NER\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# -----------------------------\n",
    "# 2. Load and Caption Image\n",
    "# -----------------------------\n",
    "image_path = \"download12.jpg\"  # Replace with your image path\n",
    "image = Image.open(image_path).convert(\"RGB\")\n",
    "\n",
    "# No prompt â€” BLIP-2 OPT works as a caption generator here\n",
    "inputs = processor(images=image, return_tensors=\"pt\").to(device)\n",
    "out = model.generate(**inputs, max_new_tokens=30)\n",
    "caption = processor.decode(out[0], skip_special_tokens=True)\n",
    "\n",
    "print(\"Generated Caption:\", caption)\n",
    "\n",
    "# -----------------------------\n",
    "# 3. Clean Caption & Tokenize\n",
    "# -----------------------------\n",
    "def clean_text(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)  # Remove punctuation\n",
    "    return text\n",
    "\n",
    "clean_caption = clean_text(caption)\n",
    "tokens = [word for word in clean_caption.split() if word not in STOP_WORDS]\n",
    "\n",
    "# -----------------------------\n",
    "# 4. Named Entity Recognition\n",
    "# -----------------------------\n",
    "doc = nlp(caption)\n",
    "ner_entities = [ent.text.lower() for ent in doc.ents if ent.label_ in [\"ORG\", \"PRODUCT\", \"GPE\", \"PERSON\"]]\n",
    "\n",
    "# -----------------------------\n",
    "# 5. Combine & Select Brand\n",
    "# -----------------------------\n",
    "candidates = list(dict.fromkeys(ner_entities + tokens))  # Deduplicate, keep order\n",
    "\n",
    "if ner_entities:\n",
    "    predicted_brand = ner_entities[0]  # Prefer NER entity\n",
    "    source = \"NER\"\n",
    "elif candidates:\n",
    "    predicted_brand = candidates[0]    # Fallback to token\n",
    "    source = \"Heuristic\"\n",
    "else:\n",
    "    predicted_brand = \"Unknown\"\n",
    "    source = \"None\"\n",
    "\n",
    "# -----------------------------\n",
    "# 6. Output Results\n",
    "# -----------------------------\n",
    "print(\"NER Entities:\", ner_entities)\n",
    "print(\"Tokens:\", tokens)\n",
    "print(\"Candidates:\", candidates)\n",
    "print(f\"Predicted Brand ({source}): {predicted_brand}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "1963fdf0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted Brand: Question: What brand is this product? Answer: louis vuitton\n"
     ]
    }
   ],
   "source": [
    "# -----------------------------\n",
    "# 2. Load Image & Prompt\n",
    "# -----------------------------\n",
    "image_path = \"download13.jpg\"\n",
    "image = Image.open(image_path).convert(\"RGB\")\n",
    "prompt = \"Question: What brand is this product? Answer:\"\n",
    "\n",
    "# -----------------------------\n",
    "# 3. Generate Caption (Brand Name)\n",
    "# -----------------------------\n",
    "inputs = processor(images=image, text=prompt, return_tensors=\"pt\").to(device)\n",
    "output = model.generate(**inputs, max_new_tokens=30)\n",
    "caption = processor.decode(output[0], skip_special_tokens=True)\n",
    "\n",
    "# -----------------------------\n",
    "# 4. Display Result\n",
    "# -----------------------------\n",
    "print(\"Predicted Brand:\", caption.strip())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "85996504",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted Brand: louis vuitton\n"
     ]
    }
   ],
   "source": [
    "mage = Image.open(\"download13.jpg\")\n",
    "prompt = \"Question: What brand is this product? Answer:\"\n",
    "\n",
    "inputs = processor(images=image, text=prompt, return_tensors=\"pt\").to(device)\n",
    "output = model.generate(**inputs, max_new_tokens=10)\n",
    "caption = processor.decode(output[0], skip_special_tokens=True)\n",
    "\n",
    "# Extract only brand name if prompt is repeated in output\n",
    "brand = caption.split(\"Answer:\")[-1].strip() if \"Answer:\" in caption else caption.strip()\n",
    "\n",
    "print(\"Predicted Brand:\", brand)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (venv)",
   "language": "python",
   "name": "venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
